---
title: 服务器磁盘io性能监控和优化
date: 2025-04-02 11:05:00
categories: [运维]
tags: [运维, 磁盘IO, 性能优化]
image:
  path: /assets/img/posts/common/ubuntu.jpg
---

# 服务器磁盘io性能监控和优化

## io性能评价指标
+ iops 每秒io请求次数，包括读和写
+ 吞吐量 每秒io流量，包括读和写

## 磁盘io性能监控工具
### 使用iostat监控各磁盘io性能
```sh
sudo apt-get install sysstat
iostat -x -k -d 1
```
`util`列反应了各磁盘io繁忙程度，值越高说明负载越大

### 使用iotop监控各进程io流量
```sh
iotop -oP
```

### 使用stats命令查看容器io
```sh
docker stats
```
关注`BLOCK I/O`列，记录了容器启动到现在的历史统计流量。

## 使用fio测试磁盘iops性能
```sh
# 测试随机写入。单线程，单次写4K的块，最大1G，运行60秒。测试生成的1G文件，会写入到当前目录下的--name指定的文件内。
# 参数：队列深度--iodepth，描述了并发处理的io请求数。可以查看和调整指定设备或者操作系统的默认队列深度；另外在iostat结果中，可以从aqu-sz: 平均 I/O 队列长度（average queue size）
# 参数：指定硬盘设备--filename=/dev/<设备名称>，可以使用lsblk查看待测试文件路径关联的设备
fio --name=random-write --ioengine=libaio --rw=randwrite --bs=4k --direct=1 --size=1G --numjobs=1 --iodepth=32 --runtime=60 --time_based --group_reporting --filename=/dev/sda

# 测试顺序写
fio --name=random-write --ioengine=libaio --rw=write --bs=4k --direct=1 --size=1G --numjobs=1 --runtime=60 --time_based --group_reporting

# 测试混合读写。一般70%读，30%写
fio --name=mixed_test --ioengine=libaio --rw=randrw --rwmixwrite=70 --bs=4k --size=1g --numjobs=1 --direct=1 --group_reporting
```
对于少读多写场景，关注`write`行的`IOPS`和`BW`吞吐量，以及`lat`行写入延迟的`avg`值

## 磁盘io优化
### 应用层优化
在中间件、数据库等应用系统层面通过增加内存缓存池、减少日志刷盘（java使用logback频繁打印日志容易产生串行锁导致高cpu占用）、减低checkpoint检查点落盘备份频率等。

### 调整磁盘io调度算法
调度器类型
+ noop 无优化，主要用于测试
+ cfq 完全公平队列，默认的调度器，适用于桌面和工作站
+ mq-deadline 适用于多队列环境
+ deadline 适用于数据库环境

```sh
# 查看磁盘挂载
lsblk

# 查看指定磁盘调度器。[]符号内是当前激活活动的配置
cat /sys/block/<磁盘名称>/queue/scheduler

# 临时修改调度器。重启会失效
echo deadline > /sys/block/<磁盘名称>/queue/scheduler
```

### 调整操作系统文件缓存参数
对于少读多写的场景，提高阈值可以让更多的数据暂存在内存中，减少频繁的小规模写入操作，让后台异步慢慢的落盘处理，从而降低磁盘I/O压力。但过高的值可能导致数据丢失风险
```sh
# 临时修改，重启失效
sysctl -a | grep dirty
sysctl -w vm.dirty_background_ratio=20
sysctl -w vm.dirty_ratio=40

# 永久修改
vi sysctl.conf
# 修改后刷新
sysctl -p
```
