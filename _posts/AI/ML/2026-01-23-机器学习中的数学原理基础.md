---
title: 机器学习中的数学原理基础
date: 2026-01-23 09:00:00
categories: [AI, ML]
tags: [AI, ML]
image:
  path: /assets/img/posts/common/ml.jpg
---

# 机器学习中的数学原理基础

> 深入理解机器学习中常用的数学概念（方差、标准差、协方差、相关性等），从第一性原理出发，理解这些概念的本质、作用和计算方法。理解数学是机器学习的基础，掌握这些数学原理能够帮助我们更好地理解机器学习算法。

## 目录

1. [为什么需要数学原理？](#1-为什么需要数学原理)
2. [描述性统计量](#2-描述性统计量)
3. [分布形状度量](#3-分布形状度量)
4. [变量关系度量](#4-变量关系度量)
5. [距离度量](#5-距离度量)
6. [概率基础](#6-概率基础)
7. [优化基础](#7-优化基础)
8. [线性代数基础](#8-线性代数基础)
9. [数学原理在机器学习中的应用](#9-数学原理在机器学习中的应用)

---

## 1. 为什么需要数学原理？

### 1.1 数学是机器学习的基础

#### 模型是数学函数的组合

**本质**：机器学习模型本质上是数学函数的组合。

**例子**：
```
线性模型：y = a₁x₁ + a₂x₂ + ... + aₙxₙ + b
    ↓
    数学函数（线性函数）
    
决策树：IF x₁ > 10 THEN ... ELSE ...
    ↓
    数学函数（分段函数）
    
神经网络：y = f(W·x + b)
    ↓
    数学函数（复合函数）
```

#### 优化是数学问题的求解

**本质**：模型训练本质上是数学优化问题。

**例子**：
```
优化目标：最小化损失函数
    min L(θ)
    ↓
    数学优化问题
    
优化方法：梯度下降
    θ = θ - α × ∇L(θ)
    ↓
    数学优化算法
```

#### 评估是数学度量的计算

**本质**：模型评估本质上是数学度量的计算。

**例子**：
```
准确率：Accuracy = 正确预测数 / 总样本数
    ↓
    数学度量
    
F1分数：F1 = 2 × (Precision × Recall) / (Precision + Recall)
    ↓
    数学度量
```

### 1.2 从第一性原理理解

#### 理解本质，而不是记忆公式

**问题**：死记硬背公式，不理解本质。

**解决**：从第一性原理理解每个概念。

**例子**：
```
不是直接记住：
    方差 = Σ(值i - 均值)² / n
    
而是理解：
    为什么需要方差？→ 衡量数据的离散程度
    为什么需要平方？→ 避免正负抵消
    为什么除以n？→ 得到平均值
```

#### 知道为什么需要这个概念

**关键**：理解每个概念解决了什么问题。

**例子**：
```
为什么需要方差？
    → 衡量数据的离散程度
    → 了解数据是否集中
    
为什么需要协方差？
    → 衡量两个变量的关系
    → 了解特征之间是否相关
```

#### 知道如何应用

**关键**：理解如何在实际问题中应用这些概念。

**例子**：
```
方差的应用：
    → 特征工程：STD(时间间隔) → 反映规律性
    → 模型评估：评估预测的不确定性
    → 异常检测：识别异常值（3σ原则）
```

---

## 2. 描述性统计量

### 2.1 中心趋势度量

#### 均值（Mean）

**定义**：所有值的平均。

**公式**：
```
均值 = (值1 + 值2 + ... + 值n) / n
```

**本质**：数据的"重心"，所有值的平衡点。

**例子**：
```
数据：[20, 25, 30, 35, 40]
均值 = (20 + 25 + 30 + 35 + 40) / 5 = 30
```

**局限性**：
- **受异常值影响大**：异常值会拉高或拉低均值
- **不适合偏态分布**：偏态分布时均值不能代表典型情况

**例子（异常值影响）**：
```
数据：[20, 25, 30, 35, 100]
均值 = (20 + 25 + 30 + 35 + 100) / 5 = 42
    ↓
    均值42被异常值100拉高
    不能代表数据的真实中心 ⚠️
    
中位数 = 30
    ↓
    中位数不受异常值影响 ✅
```

#### 中位数（Median）

**定义**：排序后中间位置的值。

**本质**：数据的"中点"。

**优势**：不受异常值影响，更稳健。

**适用场景**：
- 数据有异常值时
- 数据分布不对称时

**例子**：
```
数据：[20, 25, 30, 35, 100]
排序后：[20, 25, 30, 35, 100]
中位数 = 30（中间位置的值）
    ↓
    不受异常值100影响 ✅
```

#### 众数（Mode）

**定义**：出现最频繁的值。

**本质**：数据的"典型值"。

**适用场景**：类别型数据。

**例子**：
```
数据：['A类', 'B类', 'A类', 'A类', 'C类']
众数 = 'A类'（出现3次，最多）
```

### 2.2 离散程度度量

#### 方差（Variance）

**定义**：衡量数据偏离均值的程度。

**公式**：
```
方差 = Σ(值i - 均值)² / n
```

**本质**：
- 衡量数据偏离均值的程度
- 数值越大，数据越分散

**为什么需要平方？**

**问题1：避免正负抵消**

**例子**：
```
数据：[20, 30, 40]
均值 = 30

差值：
    20 - 30 = -10
    30 - 30 = 0
    40 - 30 = 10
    
如果直接求和：
    (-10) + 0 + 10 = 0
    ↓
    正负抵消，无法衡量离散程度 ❌
```

**问题2：放大偏离程度**

**解决**：使用平方。

**例子**：
```
差值：
    (-10)² + 0² + 10² = 100 + 0 + 100 = 200
    ↓
    避免正负抵消 ✅
    放大偏离程度 ✅
    
方差 = 200 / 3 = 66.67
```

**单位问题**：方差的单位是原始单位的平方。

**例子**：
```
原始数据单位：天
方差单位：天²（不直观）
    ↓
    因此需要标准差（√方差）
```

#### 标准差（Standard Deviation）

**定义**：方差的平方根。

**公式**：
```
标准差 = √方差
```

**本质**：
- 方差的平方根
- 与原始数据同单位，更易理解

**作用**：
- **衡量数据的离散程度**：数值越大，数据越分散
- **评估数据的稳定性**：标准差小表示数据稳定
- **识别异常值**：使用3σ原则（超出均值±3σ的数据可能是异常值）

**例子**：
```
数据：[20, 25, 30, 35, 40]
均值 = 30
方差 = 50
标准差 = √50 ≈ 7.07

含义：
    - 平均值是30
    - 标准差是7.07（离散程度）
    - 数据大致在 30 ± 7.07 范围内
```

#### 范围（Range）

**定义**：最大值 - 最小值。

**本质**：数据的跨度。

**局限性**：受异常值影响。

**例子**：
```
数据：[20, 25, 30, 35, 100]
范围 = 100 - 20 = 80
    ↓
    被异常值100拉大 ⚠️
```

#### 四分位距（IQR）

**定义**：Q3 - Q1（第三四分位数 - 第一四分位数）。

**本质**：中间50%数据的范围。

**优势**：不受异常值影响。

**例子**：
```
数据：[20, 25, 30, 35, 100]
Q1 = 22.5（25%位置）
Q3 = 67.5（75%位置）
IQR = 67.5 - 22.5 = 45
    ↓
    不受异常值100影响 ✅
```

---

## 3. 分布形状度量

### 3.1 偏度（Skewness）

#### 定义

**偏度**：衡量分布的对称性。

**公式**：
```
偏度 = Σ((值i - 均值)³ / n) / (标准差³)
```

**本质**：衡量分布的对称程度。

#### 解释

**偏度 > 0**：右偏（长尾在右）
- 大部分数据在左侧
- 少数大值在右侧

**偏度 < 0**：左偏（长尾在左）
- 大部分数据在右侧
- 少数小值在左侧

**偏度 ≈ 0**：对称分布
- 数据左右对称

**例子**：
```
右偏分布：
    数据：[10, 15, 20, 25, 100]
    偏度 > 0
    ↓
    大部分数据在左侧（10-25）
    少数大值在右侧（100）
    
对称分布：
    数据：[10, 15, 20, 25, 30]
    偏度 ≈ 0
    ↓
    数据左右对称
```

### 3.2 峰度（Kurtosis）

#### 定义

**峰度**：衡量分布的尖锐程度。

**公式**：
```
峰度 = Σ((值i - 均值)⁴ / n) / (标准差⁴) - 3
```

**本质**：衡量分布相对于正态分布的尖锐程度。

#### 解释

**峰度 > 0**：尖峰分布（比正态分布更尖锐）
- 数据集中在中心附近
- 尾部较薄

**峰度 < 0**：平峰分布（比正态分布更平缓）
- 数据分布较均匀
- 尾部较厚

**峰度 ≈ 0**：正态分布

**例子**：
```
尖峰分布：
    数据集中在中心附近
    极值较少
    
平峰分布：
    数据分布较均匀
    极值较多
```

---

## 4. 变量关系度量

### 4.1 协方差（Covariance）

#### 定义

**协方差**：衡量两个变量的共同变化。

**公式**：
```
协方差 = Σ((X_i - X_mean) × (Y_i - Y_mean)) / n
```

**本质**：
- 衡量两个变量的共同变化
- 正值：正相关（同向变化）
- 负值：负相关（反向变化）
- 零值：无相关

#### 为什么有效？

**原理**：如果两个变量相关，它们会一起变化。

**例子**：
```
数据：
    X（特征A）：[20, 25, 30, 35, 40]
    Y（特征B）：[10, 15, 20, 25, 30]
    
X_mean = 30
Y_mean = 20

协方差计算：
    (20-30)×(10-20) + (25-30)×(15-20) + ... = 正数
    ↓
    协方差 > 0（正相关）
    ↓
    特征A和特征B同向变化 ✅
```

#### 局限性

**局限1：受量纲影响，难以比较**

**问题**：不同量纲的协方差无法直接比较。

**例子**：
```
协方差1（特征A, 特征B）= 100（单位：A×B）
协方差2（特征C, 特征D）= 50（单位：C×D）
    ↓
    无法直接比较哪个相关性更强 ⚠️
```

**局限2：只反映线性关系**

**问题**：只能反映线性关系，不能反映非线性关系。

**例子**：
```
非线性关系：
    X和Y有非线性关系（如X²）
    但协方差可能为0（无线性关系）
    ↓
    协方差无法发现非线性关系 ⚠️
```

### 4.2 相关系数（Correlation Coefficient）

#### 定义

**相关系数**：标准化的协方差。

**公式**：
```
相关系数 = 协方差 / (X的标准差 × Y的标准差)
```

**本质**：
- 标准化的协方差
- 取值范围 [-1, 1]
- 不受量纲影响

#### 解释

**接近1**：强正相关
- 两个变量同向变化
- 一个变量增加，另一个也增加

**接近-1**：强负相关
- 两个变量反向变化
- 一个变量增加，另一个减少

**接近0**：无相关
- 两个变量不相关

**例子**：
```
相关系数 = 0.8
    ↓
    强正相关
    特征A和特征B高度相关 ✅
    
相关系数 = -0.6
    ↓
    中等负相关
    特征X和特征Y反向相关
    
相关系数 = 0.1
    ↓
    弱相关
    特征A和特征C相关性很弱
```

#### 在机器学习中的应用

**应用1：特征选择（去除高度相关的特征）**

**问题**：高度相关的特征信息重复（冗余）。

**解决**：去除高度相关的特征。

**方法**：
```
计算特征之间的相关系数
    ↓
    如果相关系数 > 0.9（高度相关）
    ↓
    去除其中一个特征（保留信息量大的）
```

**应用2：特征工程（发现特征交互）**

**问题**：发现特征之间的交互作用。

**解决**：分析特征之间的相关性。

**方法**：
```
计算特征之间的相关系数
    ↓
    发现高度相关的特征组合
    ↓
    构造交互特征（如 A × B）
```

---

## 5. 距离度量

### 5.1 欧氏距离（Euclidean Distance）

#### 定义

**欧氏距离**：直线距离。

**公式**：
```
距离 = √(Σ(x_i - y_i)²)
```

**本质**：两点之间的直线距离。

**例子**：
```
点1：[1, 2]
点2：[4, 6]

欧氏距离 = √((1-4)² + (2-6)²) = √(9 + 16) = √25 = 5
```

#### 适用场景

**场景1：KNN（K近邻）**

**应用**：找到最近的K个邻居。

**场景2：聚类**

**应用**：基于距离进行聚类。

**场景3：异常检测**

**应用**：远离中心的点可能是异常值。

### 5.2 曼哈顿距离（Manhattan Distance）

#### 定义

**曼哈顿距离**：城市街区距离。

**公式**：
```
距离 = Σ|x_i - y_i|
```

**本质**：两点之间的城市街区距离。

**例子**：
```
点1：[1, 2]
点2：[4, 6]

曼哈顿距离 = |1-4| + |2-6| = 3 + 4 = 7
```

#### 适用场景

**场景1：稀疏数据**

**应用**：对稀疏数据效果好。

**场景2：高维数据**

**应用**：高维数据中效果更好。

### 5.3 余弦距离（Cosine Distance）

#### 定义

**余弦距离**：基于角度的距离。

**公式**：
```
余弦距离 = 1 - 余弦相似度
余弦相似度 = (A · B) / (||A|| × ||B||)
```

**本质**：基于角度的距离，只关注方向。

#### 适用场景

**场景1：文本相似度**

**应用**：计算文档之间的相似度。

**场景2：推荐系统**

**应用**：基于相似度推荐。

**场景3：稀疏特征**

**应用**：对稀疏特征效果好。

---

## 6. 概率基础

### 6.1 概率分布

#### 正态分布（Normal Distribution）

**定义**：钟形曲线分布。

**公式**：
```
P(x) = (1 / (σ√(2π))) × e^(-(x-μ)²/(2σ²))
```

**特点**：
- 均值μ（中心位置）
- 标准差σ（分散程度）
- 钟形曲线

**应用**：
- **假设检验**：很多统计检验基于正态分布假设
- **异常检测**：3σ原则（超出均值±3σ的数据可能是异常值）
- **模型假设**：很多模型假设数据服从正态分布

**例子**：
```
数据分布：
    均值μ = 30
    标准差σ = 5
    
含义：
    - 数据集中在30附近
    - 大部分数据在 30 ± 5 范围内
    - 3σ原则：超出 30 ± 15 的数据可能是异常值
```

#### 均匀分布（Uniform Distribution）

**定义**：所有值概率相等。

**特点**：所有值概率相等。

**应用**：随机抽样、随机数生成。

#### 泊松分布（Poisson Distribution）

**定义**：描述事件发生次数。

**特点**：描述稀有事件的发生次数。

**应用**：计数型数据、事件发生频率。

### 6.2 条件概率与贝叶斯定理

#### 条件概率

**定义**：在给定条件下，事件发生的概率。

**公式**：
```
P(A|B) = P(A∩B) / P(B)
```

**本质**：在B发生的条件下，A发生的概率。

**例子**：
```
P(A类|特征X>10) = P(A类∩特征X>10) / P(特征X>10)
    ↓
    在特征X>10的条件下，A类的概率
```

#### 贝叶斯定理

**定义**：基于条件概率的定理。

**公式**：
```
P(A|B) = P(B|A) × P(A) / P(B)
```

**本质**：利用已知信息更新概率。

**应用**：朴素贝叶斯分类器。

**例子**：
```
已知：
    P(A类) = 0.1（先验概率）
    P(特征X>10|A类) = 0.8（似然）
    P(特征X>10) = 0.2（证据）
    
计算：
    P(A类|特征X>10) = P(特征X>10|A类) × P(A类) / P(特征X>10)
                     = 0.8 × 0.1 / 0.2 = 0.4
    
含义：
    在特征X>10的条件下，A类的 posterior probability（后验概率）是 0.4
```

---

## 7. 优化基础

### 7.1 梯度

#### 定义

**梯度**：函数在某点的斜率（导数）。

**公式**：
```
梯度 = ∂f/∂x（偏导数）
```

**本质**：函数增长最快的方向。

**作用**：指示函数增长最快的方向。

**例子**：
```
函数：f(x) = x²
梯度：∂f/∂x = 2x

在x=3处：
    梯度 = 2×3 = 6
    ↓
    函数在x=3处增长最快的方向是正方向（斜率6）
```

#### 应用：梯度下降

**目的**：找到函数的最小值。

**方法**：
```
参数更新：
    θ = θ - α × ∇L(θ)
    
其中：
    θ：参数
    α：学习率（步长）
    ∇L(θ)：损失函数的梯度
```

**原理**：
- 梯度指向函数增长最快的方向
- 负梯度指向函数下降最快的方向
- 沿着负梯度方向更新参数，可以最小化损失函数

### 7.2 损失函数

#### 均方误差（MSE）

**定义**：预测值与真实值之差的平方的平均值。

**公式**：
```
MSE = Σ(预测值i - 真实值i)² / n
```

**本质**：衡量预测误差的大小。

**例子**：
```
真实值：[1, 2, 3]
预测值：[1.2, 1.8, 3.1]

误差：[0.2, -0.2, 0.1]
误差²：[0.04, 0.04, 0.01]
MSE = (0.04 + 0.04 + 0.01) / 3 = 0.03
```

#### 交叉熵（Cross Entropy）

**定义**：衡量预测概率分布与真实分布的差异。

**公式**：
```
交叉熵 = -Σ(真实概率i × log(预测概率i))
```

**本质**：用于分类问题，衡量概率分布的差异。

**例子**：
```
真实标签：[1, 0]（正类）
预测概率：[0.8, 0.2]

交叉熵 = -(1×log(0.8) + 0×log(0.2)) = -log(0.8) ≈ 0.223
```

### 7.3 损失函数的作用与训练评估

#### 损失函数的三个核心作用

**作用1：定义优化目标**

**本质**：训练时模型要「最小化」什么，由损失函数给出。

**例子**：
```
回归：最小化 MSE → 让预测值尽量接近真实值
分类：最小化交叉熵 → 让预测概率分布尽量接近真实标签分布
```

**作用2：提供梯度方向**

**本质**：梯度下降需要「往哪里走、走多少」，梯度来自对损失函数求导。

**例子**：
```
∇L(θ) = 损失对参数 θ 的梯度
    ↓
参数更新：θ = θ - α × ∇L(θ)
    ↓
损失下降，模型逐步拟合数据
```

**作用3：数值上反映当前拟合程度**

**本质**：损失越小，说明在当前数据上拟合得越好（注意：只说明「拟合」，不直接说明「泛化」）。

#### 能否用损失函数评价模型训练效果？

**可以，但有明显局限。**

**能说明的**：

- **训练集损失**：模型在训练集上的拟合好坏。  
  - 一直降 → 模型在学习；  
  - 几乎不降 → 可能欠拟合或学习率等问题。
- **验证集 / 测试集损失**：在未参与训练的数据上的「平均误差」或「概率偏差」。  
  - 验证损失一起下降 → 泛化在改善；  
  - 验证损失上升而训练损失继续降 → 典型过拟合信号。

**不能完全替代的**：

- **业务指标**：我们最终关心的是准确率、F1、AUC、召回率等。  
  - 损失与这些指标**有时不一致**（例如类别不平衡时，损失可能主要被多数类主导）。  
  - 因此：**用损失指导训练过程，用业务指标做模型选择、调参和上线决策**。
- **过拟合 / 欠拟合**：要结合**训练损失 vs 验证损失**的曲线看，不能只看训练损失。

#### 训练 / 验证损失与过拟合、欠拟合

**典型情况**：

```
情况1：健康训练
    训练损失 ↓  验证损失 ↓
    → 模型在学泛化规律 ✅

情况2：过拟合
    训练损失 ↓  验证损失先 ↓ 后 ↑
    → 模型开始「死记硬背」训练集 ❌
    → 应早停、加正则、加数据等

情况3：欠拟合
    训练损失、验证损失都偏高，且下降空间小
    → 模型容量不足或特征不够 ❌
    → 应换更强模型、改进特征等
```

**实践建议**：

- 训练时同时画**训练损失**和**验证损失**曲线。
- 用**验证损失**做早停（early stopping）、超参搜索，而不是只看训练损失。
- 最终选模型、对外汇报时，以**业务指标**（F1、AUC 等）为准，损失作为辅助。

#### 损失函数与评估指标的关系

| 维度     | 损失函数                     | 评估指标（如 F1、AUC）        |
|----------|------------------------------|-------------------------------|
| 用途     | 训练时优化目标               | 模型选择、调参、业务汇报      |
| 要求     | 通常要可导，便于梯度下降     | 符合业务目标即可，可不可导都行 |
| 一致性   | 与指标可能不一致             | 直接反映业务关心什么          |
| 使用时机 | 每个 batch / epoch 都算      | 常在验证集、测试集上算        |

**小结**：损失函数驱动训练、提供梯度，并粗粒度反映拟合情况；但**训练效果**要结合训练/验证损失曲线和**业务指标**一起看，不能单靠损失下结论。

---

## 8. 线性代数基础

### 8.1 向量与矩阵

#### 向量

**定义**：一维数组。

**例子**：
```
向量：[1, 2, 3]
```

#### 矩阵

**定义**：二维数组。

**例子**：
```
矩阵：
    [1, 2, 3]
    [4, 5, 6]
    [7, 8, 9]
```

#### 在机器学习中的应用

**应用1：特征表示**

**例子**：
```
特征向量：
    [特征1, 特征2, 特征3, ...]
    ↓
    每个样本用一个向量表示
```

**应用2：模型参数**

**例子**：
```
线性模型的权重：
    [w₁, w₂, w₃, ...]
    ↓
    每个特征对应一个权重
```

### 8.2 矩阵运算

#### 点积

**定义**：两个向量的点积。

**公式**：
```
A · B = Σ(aᵢ × bᵢ)
```

**应用**：计算相似度。

**例子**：
```
向量1：[1, 2, 3]
向量2：[4, 5, 6]

点积 = 1×4 + 2×5 + 3×6 = 4 + 10 + 18 = 32
```

#### 矩阵乘法

**定义**：矩阵与矩阵的乘法。

**应用**：线性变换。

**例子**：
```
矩阵A（特征矩阵）：
    [特征1, 特征2]
    [特征1, 特征2]
    ...

矩阵B（权重矩阵）：
    [权重1]
    [权重2]
    
矩阵乘法：A × B
    ↓
    线性变换（特征变换）
```

#### 特征值分解

**定义**：将矩阵分解为特征值和特征向量。

**应用**：PCA降维。

**例子**：
```
PCA降维：
    原始特征矩阵（100维）
    ↓
    特征值分解
    ↓
    主成分（2维）
    ↓
    保留主要信息
```

---

## 9. 数学原理在机器学习中的应用

### 9.1 特征工程

#### 使用统计量提取特征

**应用**：使用均值、方差、标准差等统计量提取特征。

**例子**：
```
特征工程：
    MEAN(时间间隔) → 平均间隔（均值）
    STD(时间间隔) → 间隔规律性（标准差）
    MAX(时间间隔) → 最长间隔（最大值）
    ...
```

### 9.2 模型评估

#### 使用距离度量评估性能

**应用**：使用距离度量评估预测误差。

**例子**：
```
均方误差（MSE）：
    本质是欧氏距离的平方
    ↓
    衡量预测值与真实值的距离
```

### 9.3 模型优化

#### 使用梯度下降优化参数

**应用**：使用梯度下降优化模型参数。

**例子**：
```
梯度下降：
    θ = θ - α × ∇L(θ)
    ↓
    沿着负梯度方向更新参数
    最小化损失函数
```

### 9.4 降维

#### 使用线性代数降维

**应用**：使用特征值分解进行PCA降维。

**例子**：
```
PCA降维：
    高维特征（100维）
    ↓
    特征值分解
    ↓
    低维特征（2维）
    ↓
    保留主要信息
```

---

## 总结

本文深入介绍了机器学习中的数学原理基础。关键要点：

1. **数学是机器学习的基础**：模型、优化、评估都基于数学
2. **描述性统计量**：均值、中位数、方差、标准差等
3. **分布形状度量**：偏度、峰度
4. **变量关系度量**：协方差、相关系数
5. **距离度量**：欧氏距离、曼哈顿距离、余弦距离
6. **概率基础**：概率分布、条件概率、贝叶斯定理
7. **优化基础**：梯度、损失函数（MSE、交叉熵）、**损失函数的作用与训练评估**（能否用 loss 评价训练效果、与过拟合/欠拟合、与业务指标的关系）
8. **线性代数基础**：向量、矩阵、矩阵运算、特征值分解
9. **数学原理的应用**：特征工程、模型评估、模型优化、降维

通过理解这些数学原理，我们可以：
- 更好地理解机器学习算法
- 更好地进行特征工程
- 更好地评估和优化模型
- 更好地理解模型的行为

掌握数学原理，能够大大提高机器学习的理解和应用能力。
