---
title: AI基础-机器学习和深度学习
date: 2025-08-19 10:10:00
categories: [AI, 机器学习]
tags: [AI, 机器学习]
image:
  path: /assets/img/posts/common/AI.jpg
---

# AI基础-机器学习和深度学习
[内容部分摘自 阿里云开发者微信公众号：AI 基础知识从-1到1系列](https://mp.weixin.qq.com/s/1t7WmIPvDCqAcTdkaQqRpw)

主要涉及对象：
+ 模型（根据NLP、计算机视觉、语音处理、多模态等任务场景选择合适的预训练模型）
+ 数据集
+ 工具（清洗分拆等数据处理工具、机器学习框架等）

## AI概念层次
+ 人工智能（AI）：使用计算机模拟人类智能行为。包括专家系统、机器人、机器学习、计算机视觉等。
+ 机器学习（ML）：机器学习是人工智能的一个子集。"机器"指的是计算机，是让计算机系统能够从数据中自动学习和改进，而无需明确编程的过程。
+ 深度学习（DL）：是机器学习的一个领域。使用神经网络的多层结构来处理复杂数据模式识别。

## 数据分类
理解数据的类型和结构是至关重要。

根据特征中值的类型可以把数据分为：
- 数值数据（Numerical Data）：由数值表示的数据，可以进行数学运算，用于度量和预测具体的数值结果，比如房屋面积（平方米）、价格（万元）、房龄（年）
- 分类数据（Categorical Data）：表示类别或状态的数据，用于分类和分组任务，比如房屋所在地区（中心区、郊区）、房屋类型（独栋、联排）、销售状态（已售出、待售）

表格中的一行数据记录称之为一个数据点，标签是指模型试图预测或分类的值，也就是说每个标签对应一个数据点的结果或类别。如果我们关注数据表中行的维度，根据数据点是否包含标签可以把数据分为：
- 标签数据（Lebeled Data）：每个数据点都附带有标签的数据。
- 无标签数据（Unlabeled Data）：数据点没有标签的数据。

## 特征工程

### 特征工程的重要性
特征工程是将原始数据转换为对预测目标更有用的信息的过程。

好的特征工程往往比复杂的模型更重要，理解数据比掌握算法更关键。

### 特征工程示例
在房价预测中，真正的特征工程包括：
1. 特征创建
    - 将面积转换为每平方米价格
    - 创建位置评分（如距离市中心的距离）
    - 卧室数量与面积的比率
    - 房屋年龄的平方根（非线性变换）
2. 特征选择
    - 选择最相关的特征
    - 去除冗余特征
    - 处理缺失值
3. 特征缩放
    - 标准化（Z-score标准化）
    - 归一化（Min-Max缩放）

### 特征工程方法
扩展视频https://www.bilibili.com/video/BV1smXUYSEGi/

1. 特征编码：将非数值型特征转换为数值型特征，以便机器学习模型能够处理。比如数据集中性别特征{男性, 女性}，可以通过独热编码转换为两个新的特征：[1,0]、[0,1]。

2. 文本处理
    - 字符级分词：将文本分割为单个字符。例如，"机器学习"分割为["机", "器", "学", "习"]
    - 词级分词：将句子分割为独立的单词或词语。例如，句子"机器学习很有趣。"分割为["机器学习", "很", "有趣"]

3. 词向量（Word Embedding）：将词语转换为数值向量，使计算机能够理解词语的语义关系。

### 深度学习中的特征工程
- 传统机器学习（线性/树模型）
    - 强依赖手工特征：交叉项、分桶、目标编码、统计特征、比率/差值等。
    - 好的特征往往比模型选择更重要。
- 深度学习
    - 侧重端到端“表示学习”：模型（卷积、注意力、Transformer）从原始数据中自动提取层级特征。
    - 仍需要“轻量特征工程/数据工程”：
        - 图像：尺寸/通道规范化、标准化、数据增强（翻转、裁剪、ColorJitter、MixUp/CutMix、RandAugment）。
        - NLP：分词/子词（BPE/WordPiece）、大小写/标点规范化、构建词表/子词表、截断/填充、位置编码策略。
        - 等等。

## 机器学习
### 机器学习算法
#### “回归”的含义
回归（Regression）是指预测连续数值型输出的机器学习任务。与分类（Classification）不同，分类预测的是离散的类别标签。
```
“回归”这个词来源于统计学，最早由英国统计学家弗朗西斯·高尔顿（Francis Galton）提出。他在研究父子身高关系时发现：
高个子的父亲，儿子的身高会"回归"到平均水平
矮个子的父亲，儿子的身高也会"回归"到平均水平
这种现象被称为"回归到均值"（regression to the mean），后来"回归"就成了预测连续值的统计方法的代名词。
```

#### 线性回归
机器学习中的线性回归正是基于线性函数构建的模型，其核心是通过线性函数对输入特征和输出目标之间的关系进行建模。

在房价预测，假设目标变量（标签，房价）与一个或多个特征之间存在线性关系：
```
y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε
```

其中：
- `y` 是房价（目标变量）
- `x₁, x₂, ..., xₙ` 是特征（面积、卧室数量、位置等）
- `β₀, β₁, β₂, ..., βₙ` 是模型参数（权重）
- `ε` 是误差项

线性回归的数学表达：试图找到一条直线（或超平面）来拟合数据。参数即权重与偏置，是模型学习和预测的关键要素。

还有很多其他方式，比如：
1. 多项式回归：
二次方程、三次方程
2. 非线性变换：
对数变换、指数变换、平方根变换等。
3. 等等。

线性回归虽然简单，但在特征工程得当的情况下，往往能取得不错的效果。复杂模型不一定总是更好，关键是要理解数据的特点和业务需求。

### 机器学习类型

#### 监督学习（Supervised Learning）
监督学习是机器学习的一个分支，其目标是学习特征与标签之间的映射关系，以便在新数据上进行准确的预测。

比如安全帽识别的模型训练，在有安全帽的图片上标注打标签，喂给模型掌握这类物品的识别能力。

##### 监督学习的特点
- 准确性高：由于有明确的标签指导，模型通常在预测和分类任务中表现出较高的准确性。
- 可解释性强：模型的输出可以直接与标签对比，便于理解和解释模型的决策过程。
- 广泛应用：适用于各种实际问题，如图像识别、语音识别、医疗诊断、金融预测等。

#### 无监督学习（Unsupervised Learning）
无监督学习是一种处理无标签数据的机器学习分支，其目标是发现数据中潜在的结构，无需预先定义的标签。

##### 无监督学习的特点
- 发现隐藏模式：能够揭示数据中难以察觉的隐藏模式或结构。
- 数据探索：适用于初步的数据分析和探索，帮助理解数据的内在关系。
- 数据预处理：如降维方法，可以优化数据，使后续的监督学习模型更加高效。

##### 无监督学习应用
- **聚类**：将相似的数据分组
- **降维**：减少数据维度，保留重要信息
- **异常检测**：发现数据中的异常模式
- **数据预处理**：优化数据，使后续的监督学习模型更加高效

#### 半监督学习
半监督学习结合了监督学习和无监督学习的特点，使用少量有标签数据和大量无标签数据进行训练。

很多任务其实需要监督学习和无监督学习混合使用，比如数百万商品图片数据集无标签，需分类为 “服饰”“电子产品” 等，可以按照这样的步骤求解：
1.用无监督学习对图片聚类，发现潜在类别（如相似颜色 / 形状的商品）
2.人工标注聚类结果，数据有了标签
3.监督学习模型训练，自动化分类网站服饰、电子产品图片

#### 强化学习（Reinforcement Learning, RL）
强化学习是机器学习的一个重要分支，主要研究智能体如何在环境中通过试错学习策略，以最大化累计奖励。

##### 强化学习特点
- **无需监督信号**：不同于监督学习依赖标注数据，强化学习仅通过环境提供的奖励信号调整策略
- **试错学习**：通过与环境交互不断尝试和改进
- **延迟奖励**：奖励可能不是立即获得的，需要长期规划

##### 强化学习应用
- 游戏AI
- 汽车自动驾驶
- 机器人控制
- 推荐系统

## 深度学习

### 和机器学习的区别
传统机器学习在处理复杂、非结构化数据（如图像、音频、文本）时遇到了挑战，特别是传统机器方法需要大量人工设计特征，耗时且需要领域知识，模型难以捕捉数据中的深层关联。

深度学习引入了深层神经网络、反向传播、卷积和循环网络等新思路，实现了从原始数据中自动提取层次化特征，极大减少了对人工特征工程的依赖。其在图像识别、语音识别、自然语言处理及生成任务中表现出色，尤其擅长处理非线性关系和大规模数据。同时GPU和分布式计算技术的成熟，为训练深层神经网络提供了算力支持。

### 神经网络层次结构
- 输入层：负责接收数据输入，通常以特征形式呈现，每个神经元对应一个特征。
- 隐藏层：进行数据转换和特征提取，通过加权处理输入特征，生成更高层次的抽象特征。（这步和传统机器学习的特征提取流程不同）
- 输出层：生成预测结果，每个神经元对应一个输出变量。

### 卷积
在早期，计算机视觉依赖手工设计特征，手工设计的"经验性"体现在对图片人为的划分子区域（比如人脸的上半和下半部分）等。另外传统深度学习处理图像时存在输入层参数爆炸（图片像素太大）

卷积利用局部性原理，减少入参的参数量，结合多层操作减少特征图尺寸。

#### 不同大小卷积核的作用
+ 小卷积核（3×3）：优势参数少，计算效率高。用途检测边缘、纹理等低级特征。
+ 大卷积核：优势感受野大，能检测更大范围的特征。用途检测形状、模式等中级特征。

可以混合使用不同大小的卷积核。通过多层小卷积核等效于大卷积核。

#### 特征图尺寸与语义层次的关系
+ 浅层：大特征图，低级特征。像素级特征（边缘、纹理）
+ 中层：中等特征图，部件级特征（眼睛、轮廓）
+ 深层：对象级特征（人脸、汽车）
+ 顶层：小特征图，语义级特征（表情、动作）

深层特征图变小，计算量显著减少；感受野逐步扩大，深层网络可以"看到"整个图像的信息；抽象程度递增。

## 模型
模型就是机器学习、深度学习中从数据中学习到的、用于做预测或决策的规则集合。

### 模型框架
是一个对模型架构的工具实现。比如TensorFlow是由Google开发的开源机器学习框架，提供操作、函数和类来实现Transformer模型架构，进行模型训练和推理。而PyTorch框架相对轻量。

### 模型架构
不同类型的模型架构有不同的擅长领域场景。

模型架构，允许开发者根据需求调整细节（如层数、参数），从而衍生出无数具体实现；而GPT则是基于Transformer蓝图构建并训练完成的「成品模型」，是前者的一个典型实例。
+ 卷积神经网络 (Convolutional Neural Networks, CNNs)：通过卷积操作高效提取数据的局部空间特征，特别适用于具有网格状结构的数据，如图像。
+ 循环神经网络 (Recurrent Neural Networks, RNNs)：通过其循环结构处理序列数据，能够捕捉时间上的依赖关系，适用于需要处理时序信息的任务。
+ Transformer 模型： 基于自注意力机制，能够并行处理整个序列，擅长捕捉长距离依赖关系，广泛应用于 NLP 和计算机视觉领域。
+ 生成对抗网络 (Generative Adversarial Networks, GANs)：通过生成器与判别器的对抗训练，实现高质量数据的生成，广泛应用于生成任务和数据增强。
+ 等等。

### 模型复杂度
模型复杂度是一个用来衡量模型在结构、参数等方面复杂程度的指标，用来反映模型对数据的拟合与泛化之间的平衡关系。
+ 欠拟合（Underfitting）：模型在训练数据和测试数据上都表现不佳，通常是由于模型过于简单。
+ 过拟合（Overfitting）：模型在训练数据上表现良好，但在未见过的测试数据上表现较差，通常是由于模型过于复杂。

+ 正则化：是用于防止模型过拟合、提高模型泛化能力的技术。基本思想是在模型的损失函数中添加一个正则化项，对模型的复杂度进行惩罚，从而限制模型的参数取值范围，使得模型更加简单平滑，减少对训练数据的过拟合。

### 模型训练和调优
软件工程中落地实现的技巧细节。

#### 预训练模型使用和微调
从零开始训练模型的情况已越来越少。大多数开发者会基于任务需求与数据特性，站在巨人肩膀上，优先选择在预训练模型（Pre-trained Models）基础上进行微调优化。既充分利用了大规模数据预训练积累的通用特征表示，又能通过轻量化的适配过程快速满足特定场景需求。

预训练模型一般使用通用数据集训练，为了更好的支持业务需求，开发者会对预训练模型进行微调。微调是指在预训练模型上，针对特定任务或特定数据集，进行少量参数的调整和训练，以使模型更好地适应新的任务需求。

微调（Fine-tuning，FT）策略：
+ 标准：基于特定需求场景，使用特定任务的数据集，进行全量参数的进一步训练。
+ 监督：利用有标注数据进行进一步训练。
+ 知识蒸馏（Knowledge Distillation）：模型压缩，进一步提升模型在特定任务上的表现。

#### 数据集划分
+ 训练集：训练集是用于训练模型的数据，一般占总数据集的 60%-80%。
+ 验证集：验证集不参与模型的训练，而是用来评估模型在未见过的数据上的表现，一般占总数据集的 10%-20%。
+ 测试集：模拟真实世界的未知数据，用于最终评估模型的泛化能力，一般占总数据集的 10%-20%。

使用随机森林算法构建模型时，还会从原始训练集随机地抽取若干子样本，建立多个数据集。

#### 超参数
模型参数（权重和偏置）是在训练过程中通过数据自动学习得到的。而超参数则是开发者根据经验、实验或者一些启发式方法在模型训练之前手动设置的。用于控制整个训练过程和模型结构。比如：
+ 学习率：控制模型的权重更新幅度
+ 正则化参数
+ 批次大小：每次训练模型时使用的数据样本数量，较大的批次可以提高训练效率。
+ 隐藏层数量和单元：在神经网络中决定架构的复杂性。

#### 交叉验证
一种评估和选择模型的技术。

通过把数据集划分成多个子集，让每个子集都轮流作为验证集，其余的数据作为训练集，从而可以对模型在不同数据划分下的表现进行充分的评估。

这种方法能够充分利用数据，每个样本既参与训练又参与验证，最终将多个验证结果汇总，给出更稳定、更可靠的模型性能估计。

有框架工具封装了手动调整参数的操作步骤，并记录评估结果。GridSearchCV 是 scikit-learn 库中用于超参数调优的工具，其主要作用是根据提供的参数字典，生成所有可能的超参数组合，帮助自动遍历，通过交叉验证来寻找最佳的参数设置，从而优化模型性能。

#### 优化改进方向
- 数据层面：
    - 获取更多数据：提升泛化能力。
    - 处理异常值：识别并处理数据中的异常值，以减少其对模型的负面影响。
- 特征层面：
    - 生成新特征：如果某些场景效果不好，可以针对这类场景生成一个新的特征，如是否小地块等。
    - 特征选择：去除低相关性或冗余的特征，简化模型。
    - 特征转换：针对数据的特点改用其他变换方式。
- 模型层面：尝试其它模型、进一步超参数调优等。

部署后的模型需要持续监控其表现，并根据实际反馈进行改进。

#### 深度神经网络训练
- 前向传播：在每层的数据输入输出端使用技巧做优化
    - 激活函数：每一层先做线性变换，得到的结果使用激活函数转换，作为下一层的输入。激活函数（如 ReLU、Sigmoid、Tanh 等）引入了非线性特性，可以使得模型更复杂。
    - Dropout：随机丢弃，人为制造随机扰动。
    - 归一化：在网络的每一层输入端，强行将数据的分布重新拉回到一个稳定、标准的范围内。
    - 残差：[详见-残差连接](#残差连接)
- 反向传播：把在终点产生的误差，按照原路“归因”或“分摊”给每一层的每一个参数，搞清楚每个参数对最终的总误差贡献了多少“责任”。
    - 损失函数：衡量前向传播的预测结果与真实标签之间的差距，用数值表示。
    - 梯度下降：使损失函数（偏差）最小的模型参数，然后用来更新模型参数。

#### 残差连接
只需要学习和传递delta变化的部分，而不是全部。
```
想象我们有一个目标任务——让团队成员接力撰写一篇高质量的论文。传统做法是由第一个人起草初稿，然后传递给下一个人，每个人阅读理解后，用自己的表述重写再传递下去。然而，随着传递次数增多，最终的文章可能已经偏离了最初的主题。表现出色的前几段内容会被后续重写的修改削弱，甚至可能导致整体质量下降。

一种改进方式是：每个人只需在上一位同事的基础上补充自己的修改意见，而无需完全重写。如果发现前面的内容已经非常优秀，可以选择不作任何改动。在这种策略下，文章的整体质量能够随着参与人数的增加而逐步提高，而不会因为频繁重写而被破坏。

这也正是残差连接的核心思想：与其让网络层直接学习一个复杂的、从输入到输出的完整映射，不如让它学习输入和输出之间的“残差”或“差异”（对论文的修改意见）。
```

#### 注意力机制和多头
让模型学会自己"找重点"。
+ 自动分配权重：权重不是人工设定的，是模型自动学习得到的
+ 动态分配权重：对于每个词而言，它的注意力分布都不同
```text
# 当模型处理"我"这个词时，计算它对所有词的注意力权重：
"我"的注意力分布：
- 对"我"自己：0.3
- 对"爱"：0.4
- 对"吃"：0.2
- 对"苹果"：0.1

# 当模型处理"吃"这个词时：
"吃"的注意力分布：
- 对"我"：0.1
- 对"爱"：0.2
- 对"吃"自己：0.3
- 对"苹果"：0.4  # 最高！因为"吃"和"苹果"关系最密切
```
多头：让模型有多个"感官"，从不同角度理解信息。并行计算，多个头同时工作，效率高，全面理解。

假设有4个注意力头（Head）
```
# Head 1：关注语法关系（主谓宾结构）
"我"的注意力分布（语法视角）：
- 对"我"：0.4    # 主语，关注自己
- 对"爱"：0.4    # 谓语，语法关系密切
- 对"吃"：0.1    # 动词，关系较远
- 对"苹果"：0.1  # 宾语，关系最远

# Head 2：关注语义关系（词义相似性）
"我"的注意力分布（语义视角）：
- 对"我"：0.3    # 自己
- 对"爱"：0.2    # 情感词
- 对"吃"：0.2    # 动作词
- 对"苹果"：0.3  # 名词，与"我"都是实体

# Head 3：关注位置关系（前后顺序）
"我"的注意力分布（位置视角）：
- 对"我"：0.3    # 当前位置
- 对"爱"：0.3    # 下一个位置
- 对"吃"：0.2    # 第三个位置
- 对"苹果"：0.2  # 最后位置

# Head 4：关注其他特征（比如词性、长度等）
"我"的注意力分布（其他视角）：
- 对"我"：0.3    # 代词
- 对"爱"：0.3    # 动词
- 对"吃"：0.2    # 动词
- 对"苹果"：0.2  # 名词
```
