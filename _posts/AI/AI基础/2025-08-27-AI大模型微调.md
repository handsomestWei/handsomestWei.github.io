---
title: AI大模型微调
date: 2025-08-27 17:00:00
categories: [AI, AI基础]
tags: [AI, AI基础, 大模型微调]
image:
  path: /assets/img/posts/common/AI.jpg
---

# AI大模型微调

## 微调的作用
- 适应私有知识和数据
    - 让通用模型变成从通才变成专才。
    - 无需每次都通过 RAG（检索增强）传入上下文，降低延迟和成本。

- 优化推理效率与成本，提供模型推理效率
    - 极大减少输入token数量，降低推理成本。
        ```text
        在不微调模型的情况下，为了让大模型生成特定风格的文本，通常需要设计非常复杂的提示词。微调后把原本写在提示词里的“风格指令”固化到模型参数中。把原本写在提示词工程prompt里的“规则”和“风格”，通过训练“烧录”到模型内部，从而实现输入极简、输出高质量、成本更低、风格统一的效果，无需再通过长prompt引导是XX角色等。
        ```

- 纠正模型行为
    - 微调不仅可以“教”模型做什么，还可以“教”它不要做什么
    - 可实现的行为控制：
        - 减少幻觉：训练模型只基于给定上下文回答。
        - 避免有害输出：过滤暴力、歧视性语言。
        - 遵循指令格式：强制输出 JSON、Markdown、XML 等结构化格式。
        - 保持中立立场：避免政治、宗教倾向性表达。

- 实现多任务或多风格切换
    - 通过训练多个LoRA模块，可以让同一个基础模型“一键切换”不同能力。

## 涉及技术
### 监督微调（Supervised Fine-Tuning, SFT）
- 原理：准备一批高质量“输入-输出”样本
- 训练过程：让模型学习从短输入直接生成目标风格的输出。
- 结果：模型“记住”了这种风格，无需再通过长 prompt 引导。

### LoRA（Low-Rank Adaptation）
- 问题：全参数微调成本高、难部署。
- 解决方案：使用 LoRA、QLoRA 等技术，只训练少量新增参数（矩阵分解，低秩矩阵），冻结原模型大部分参数。
- 优势：训练快，节省计算资源，提高训练效率；让大模型快速适应新任务；可轻松切换不同风格（换 LoRA 权重即可）

#### LoRA使用
LoRA的本质，是模型参数的“增量更新”。本身不能作为小模型独立使用，必须依附于基座模型。微调后输出一个体积很小的增量更新的权重文件。使用示例：
```python
# 方式一：合并 LoRA 权重到原模型（适合独立部署）
model = model.merge_and_unload()  # 合并 LoRA 权重
model.save_pretrained("./merged_model")
tokenizer.save_pretrained("./merged_model")
```
```python
# 方式二：直接加载 LoRA 权重进行推理（推荐，灵活）
base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-7B-Instruct")
model = PeftModel.from_pretrained(base_model, "./lora-finetuned-wan21/checkpoint-500")
```
