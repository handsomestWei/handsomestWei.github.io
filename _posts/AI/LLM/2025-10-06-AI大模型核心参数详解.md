---
title: AI大模型核心参数详解
date: 2025-10-06 22:10:00
categories: [AI, LLM]
tags: [AI, LLM]
image:
  path: /assets/img/posts/common/llm.jpg
---

# AI大模型核心参数详解

> 本文档详细介绍大语言模型（LLM）推理时的核心参数（超参数），包括作用原理、使用技巧和最佳实践。

---

## 目录

- [1. 什么是超参数？](#1-什么是超参数)
  - [1.1 超参数的使用时机](#11-超参数的使用时机)
  - [1.2 通用API接口规范](#12-通用api接口规范)
- [2. 长度控制参数](#2-长度控制参数)
  - [2.1 max_new_tokens](#21-max_new_tokens-)
  - [2.2 max_length](#22-max_length)
  - [2.3 min_length](#23-min_length)
- [3. 采样策略参数](#3-采样策略参数)
  - [3.1 do_sample](#31-do_sample)
  - [3.2 temperature（温度）](#32-temperature温度-)
  - [3.3 top_p（核采样）](#33-top_p核采样nucleus-sampling-)
  - [3.4 top_k（Top-K采样）](#34-top_ktop-k采样-)
  - [3.5 采样策略对比](#35-采样策略对比)
- [4. 质量控制参数](#4-质量控制参数)
  - [4.1 repetition_penalty](#41-repetition_penalty重复惩罚-)
  - [4.2 no_repeat_ngram_size](#42-no_repeat_ngram_size)
- [5. 束搜索参数](#5-束搜索参数)
  - [5.1 num_beams](#51-num_beams束搜索宽度)
- [6. 特殊Token参数](#6-特殊token参数)
  - [6.1 eos_token_id](#61-eos_token_id结束标记)
  - [6.2 pad_token_id](#62-pad_token_id填充标记)
- [7. 参数组合策略](#7-参数组合策略)
- [8. 参数调优实战技巧](#8-参数调优实战技巧)
- [9. 参数快速参考](#9-参数快速参考)

---

## 1. 什么是超参数？

**超参数（Hyperparameters）** 是在模型训练或推理前设定的参数，用于控制模型的行为。与模型参数（权重、偏置）不同，超参数不是通过训练学习得到的，而是由用户手动设置。

```
模型参数：通过训练数据学习得到（如权重、偏置）
超参数：用户设置，控制生成行为（如temperature、top_p）
```

### 核心参数分类

```python
generation_kwargs = dict(
    # 1. 长度控制参数
    max_new_tokens=2048,
    max_length=4096,
    min_length=10,
    
    # 2. 采样策略参数 ⭐ 最重要
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    top_k=50,
    
    # 3. 质量控制参数
    repetition_penalty=1.1,
    no_repeat_ngram_size=3,
    
    # 4. 束搜索参数
    num_beams=1,
    num_beam_groups=1,
    
    # 5. 特殊token
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
)
```

---

## 1.1 超参数的使用时机

### 何时设置超参数？

**答案：每次生成时传递，而不是模型加载时！** ⭐

```python
# ❌ 错误理解：在模型加载时设置
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    temperature=0.7,  # ❌ 模型加载不支持这些参数
    top_p=0.9         # ❌ 这样写会报错
)

# ✅ 正确做法：在generate()时传递
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct"  # 只加载模型
)

# 每次生成时传递超参数
output = model.generate(
    input_ids,
    max_new_tokens=2048,      # ✅ 在这里设置
    temperature=0.7,          # ✅ 在这里设置
    top_p=0.9,                # ✅ 在这里设置
    top_k=50,
    repetition_penalty=1.1,
)
```

### 设置时机对比

#### 模型加载时（固定参数）

```python
# 这些是模型加载的参数（与超参数不同）
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    device_map="auto",        # 设备映射
    torch_dtype=torch.float16,  # 数据类型
    trust_remote_code=True,   # 信任远程代码
    low_cpu_mem_usage=True,   # 低内存模式
)
# 这些参数一次性设置，整个运行期间不变
```

#### 每次生成时（动态参数）⭐

```python
# 超参数每次对话都可以不同
# 第一次对话：需要准确的回答
response1 = model.generate(
    input_ids1,
    temperature=0.3,          # 低温度，更准确
    top_p=0.85,
    max_new_tokens=512,
)

# 第二次对话：需要创意的回答
response2 = model.generate(
    input_ids2,
    temperature=0.9,          # 高温度，更创意
    top_p=0.95,
    max_new_tokens=2048,
)

# 可以根据每次对话的需求灵活调整！✅
```

### 为什么要每次传递？

#### 原因1：灵活性

```python
# 根据不同任务动态调整
if task_type == "事实问答":
    params = {"temperature": 0.3, "top_p": 0.85}
elif task_type == "创意写作":
    params = {"temperature": 0.9, "top_p": 0.95}
elif task_type == "代码生成":
    params = {"temperature": 0.2, "do_sample": False}

output = model.generate(input_ids, **params)
```

#### 原因2：用户控制

```python
# 用户可以通过UI实时调整
user_temperature = slider_value  # 用户拖动滑块
output = model.generate(
    input_ids,
    temperature=user_temperature,  # 使用用户设置的值
    ...
)
```

#### 原因3：A/B测试

```python
# 测试不同参数的效果
for temp in [0.5, 0.7, 0.9]:
    output = model.generate(
        input_ids,
        temperature=temp,
        ...
    )
    evaluate(output)  # 比较不同温度的效果
```

---

## 1.2 通用API接口规范

### HuggingFace Transformers 标准（最常用）⭐

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# 加载模型
model = AutoModelForCausalLM.from_pretrained("model_name")
tokenizer = AutoTokenizer.from_pretrained("model_name")

# 生成 - 标准接口
output = model.generate(
    input_ids,                    # 必需：输入token
    attention_mask=attention_mask, # 推荐：注意力掩码
    
    # 长度控制
    max_new_tokens=2048,
    max_length=4096,
    min_length=10,
    
    # 采样策略
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    top_k=50,
    
    # 质量控制
    repetition_penalty=1.1,
    no_repeat_ngram_size=0,
    
    # 束搜索
    num_beams=1,
    num_return_sequences=1,
    
    # 特殊token
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
    
    # 流式输出
    streamer=streamer,  # 可选
)
```

### OpenAI API 规范

```python
import openai

# OpenAI API调用
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ],
    
    # 参数名称略有不同
    max_tokens=2048,           # 等同于max_new_tokens
    temperature=0.7,           # 相同
    top_p=0.9,                 # 相同
    # ⚠️ OpenAI不支持top_k
    
    frequency_penalty=0.0,     # 类似repetition_penalty
    presence_penalty=0.0,      # 新出现词的惩罚
    
    stream=True,               # 流式输出
)
```

### 各平台API对比

| 参数 | HuggingFace | OpenAI | Anthropic (Claude) | 说明 |
|------|------------|--------|-------------------|------|
| 长度控制 | `max_new_tokens` | `max_tokens` | `max_tokens_to_sample` | 名称不同 |
| 温度 | `temperature` | `temperature` | `temperature` | ✅ 通用 |
| 核采样 | `top_p` | `top_p` | `top_p` | ✅ 通用 |
| Top-K | `top_k` | ❌ 不支持 | `top_k` | OpenAI无 |
| 重复惩罚 | `repetition_penalty` | `frequency_penalty` | ❌ 不支持 | 名称不同 |
| 流式输出 | `streamer` | `stream=True` | `stream=True` | 实现不同 |

### 本地部署 vs API调用

#### 本地部署（更灵活）

```python
# 完全控制所有参数
output = model.generate(
    input_ids,
    max_new_tokens=2048,
    temperature=0.7,
    top_p=0.9,
    top_k=50,                    # ✅ 支持所有参数
    repetition_penalty=1.1,
    no_repeat_ngram_size=3,      # ✅ 细粒度控制
    num_beams=1,
    # ... 更多参数
)
```

#### API调用（受限但简单）

```python
# OpenAI API - 参数较少
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=messages,
    max_tokens=2048,
    temperature=0.7,
    top_p=0.9,
    # ❌ 不支持top_k
    # ❌ 不支持no_repeat_ngram
    # ❌ 不支持num_beams
)
```

### 实际项目中的使用方式

#### 方式1：直接传递（简单场景）

```python
def chat(user_input):
    input_ids = tokenizer.encode(user_input, return_tensors="pt")
    
    output = model.generate(
        input_ids,
        max_new_tokens=2048,
        temperature=0.7,
        top_p=0.9,
        top_k=50,
        repetition_penalty=1.1,
    )
    
    return tokenizer.decode(output[0])
```

#### 方式2：配置字典（推荐）⭐

```python
# 预定义配置
DEFAULT_CONFIG = {
    "max_new_tokens": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
    "repetition_penalty": 1.1,
    "do_sample": True,
}

def chat(user_input, custom_params=None):
    # 合并默认配置和自定义参数
    params = {**DEFAULT_CONFIG, **(custom_params or {})}
    
    input_ids = tokenizer.encode(user_input, return_tensors="pt")
    output = model.generate(input_ids, **params)
    
    return tokenizer.decode(output[0])

# 使用默认配置
response1 = chat("你好")

# 覆盖部分参数
response2 = chat("写一首诗", {"temperature": 0.9, "max_new_tokens": 1024})
```

#### 方式3：场景模板（最佳实践）⭐⭐

```python
# 定义场景模板
SCENE_CONFIGS = {
    "对话": {
        "max_new_tokens": 2048,
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 50,
        "repetition_penalty": 1.1,
    },
    "问答": {
        "max_new_tokens": 512,
        "temperature": 0.3,
        "top_p": 0.85,
        "top_k": 30,
        "repetition_penalty": 1.05,
    },
    "创作": {
        "max_new_tokens": 4096,
        "temperature": 0.9,
        "top_p": 0.95,
        "top_k": 80,
        "repetition_penalty": 1.2,
    },
}

def chat(user_input, scene="对话", **override):
    # 获取场景配置
    params = SCENE_CONFIGS[scene].copy()
    # 允许覆盖
    params.update(override)
    
    input_ids = tokenizer.encode(user_input, return_tensors="pt")
    output = model.generate(input_ids, **params)
    
    return tokenizer.decode(output[0])

# 使用场景配置
response1 = chat("你好", scene="对话")
response2 = chat("什么是量子力学？", scene="问答")
response3 = chat("写一首诗", scene="创作", temperature=1.0)  # 覆盖温度
```

### 关键总结

**Q1: 超参数何时设置？**
- ✅ **每次调用generate()时传递**
- ❌ 不是在模型加载时设置

**Q2: 可以动态修改吗？**
- ✅ **完全可以**，每次对话都能使用不同参数
- ✅ 这正是超参数的优势

**Q3: 有统一规范吗？**
- ✅ **HuggingFace Transformers是事实标准**
- ⚠️ OpenAI/Claude等API略有差异（主要是命名）

**Q4: 本项目如何使用？**
- ✅ 通过Gradio UI滑块让用户调整
- ✅ 每次对话时传递给model.generate()
- ✅ 支持实时修改，无需重启

---

## 2. 长度控制参数

### 2.1 max_new_tokens ⭐⭐⭐⭐⭐

**作用：** 控制本次生成的新token数量上限

```python
max_new_tokens=2048  # 最多生成2048个新token
```

**特点：**
- 不包括输入长度
- 达到限制后立即停止
- 是最直接的长度控制手段

**推荐值：**
- 短对话：256-512
- 长文本：2048-4096

#### 工作原理图解

```
┌─────────────────────────────────────────────────────┐
│  步骤1：准备输入 (input_ids)                         │
├─────────────────────────────────────────────────────┤
│ 系统提示词 (200 tokens)                              │
│ + 历史对话 (1000 tokens)                             │
│ + 当前用户输入 (100 tokens)                          │
│ ─────────────────────────────                       │
│ = 总输入：1300 tokens                                │
└─────────────────────────────────────────────────────┘
                    ↓
            [模型处理过程]
                    ↓
┌─────────────────────────────────────────────────────┐
│  步骤2：模型生成                                     │
├─────────────────────────────────────────────────────┤
│ 输入部分（1-1300 tokens）        ← 已有的           │
│ + 新生成（1301-3348 tokens）     ← max_new_tokens   │
│                                   = 2048个新token    │
│ ─────────────────────────────                       │
│ = 总输出：3348 tokens                                │
└─────────────────────────────────────────────────────┘
                    ↓
           [skip_prompt=True]
                    ↓
┌─────────────────────────────────────────────────────┐
│  步骤3：返回结果                                     │
├─────────────────────────────────────────────────────┤
│ 输入部分(1-1300): 跳过 ✗                            │
│ 新生成(1301-3348): 返回给用户 ✓                     │
└─────────────────────────────────────────────────────┘
```

#### 关键区别强调

```python
❌ 常见误解：
max_new_tokens=2048 表示输出总共2048个token

✅ 正确理解：
max_new_tokens=2048 表示在输入基础上再生成2048个token

实例：
输入：1500 tokens（历史对话 + 当前问题）
设置：max_new_tokens=2048
新生成：最多2048个token
总长度：1500 + 2048 = 3548 tokens
```

### 2.2 max_length

**作用：** 控制输入+输出的总长度

```python
max_length=4096  # 输入+输出总共不超过4096个token
```

**与max_new_tokens的区别：**

```python
# 场景1：使用max_new_tokens
输入：1000 tokens
max_new_tokens=2048
结果：最多生成2048个新token，总长度3048

# 场景2：使用max_length
输入：1000 tokens
max_length=4096
结果：最多生成3096个新token（4096-1000）

# ⚠️ 推荐使用max_new_tokens，更直观
```

### 2.3 min_length

**作用：** 强制生成的最小长度

```python
min_length=50  # 至少生成50个token才能停止
```

**使用场景：**
- 确保回答足够详细
- 避免过早终止

---

## 3. 采样策略参数 ⭐⭐⭐⭐⭐

这是**最重要的一组参数**，决定了模型生成的随机性和质量。

### 3.1 do_sample

**作用：** 是否启用随机采样

```python
# 贪心解码（确定性）
do_sample=False  # 每次都选概率最高的词
输入："今天天气"
输出："很好" （固定输出，每次都一样）

# 随机采样（多样性）
do_sample=True   # 根据概率分布随机选择
输入："今天天气"
输出可能1："很好"
输出可能2："真不错"
输出可能3："挺好的"
```

**推荐：**
- ✅ **对话应用：do_sample=True**（更自然、有趣）
- ❌ 代码生成：do_sample=False（需要确定性）

### 3.2 temperature（温度） ⭐⭐⭐⭐⭐

**作用：** 控制输出的随机性和创造性

**原理：** 调整概率分布的平滑程度

```python
# 原始概率分布
词汇概率 = {
    "好": 0.5,
    "不错": 0.3,
    "真好": 0.15,
    "太棒": 0.05
}

# temperature=0.1（趋近确定）
调整后 = {
    "好": 0.95,      # 几乎总是选这个
    "不错": 0.04,
    "真好": 0.009,
    "太棒": 0.001
}

# temperature=1.0（标准）
调整后 = {
    "好": 0.5,       # 保持原分布
    "不错": 0.3,
    "真好": 0.15,
    "太棒": 0.05
}

# temperature=2.0（非常随机）
调整后 = {
    "好": 0.35,      # 更平均分布
    "不错": 0.28,
    "真好": 0.22,
    "太棒": 0.15
}
```

**参数范围与效果：**

| 值 | 效果 | 输出特点 | 适用场景 |
|---|------|---------|---------|
| 0.1-0.3 | 非常确定 | 重复、保守、准确 | 事实问答、翻译 |
| 0.4-0.6 | 较确定 | 稳定、可靠 | 代码生成、技术文档 |
| **0.7-0.9** | **平衡** | **自然、多样** | **日常对话** ⭐ |
| 1.0-1.2 | 较随机 | 有创意、多样性高 | 创意写作 |
| 1.5-2.0 | 非常随机 | 天马行空、可能离题 | 头脑风暴 |
| >2.0 | 混乱 | 不连贯、无意义 | ❌ 不推荐 |

**实际示例：**

```python
输入："写一个关于AI的标题"

temperature=0.1:
"AI技术发展概述"（总是这个）

temperature=0.7:
"AI技术发展概述"
"人工智能的未来展望"
"探索AI的无限可能"

temperature=1.5:
"当AI遇见诗歌：一场跨越时空的对话"
"AI会梦见电子羊吗？"
"从0到1：AI如何改变世界"
```

### 3.3 top_p（核采样，Nucleus Sampling） ⭐⭐⭐⭐⭐

**作用：** 保留累计概率达到p的最小候选集

**原理：** 动态选择候选词数量

```python
# 词汇概率（从高到低排序）
候选词 = [
    ("好", 0.4),      # 累计：0.4
    ("不错", 0.25),   # 累计：0.65
    ("真好", 0.2),    # 累计：0.85
    ("挺好", 0.1),    # 累计：0.95  ← top_p=0.9在这里截断
    ("棒", 0.03),
    ("赞", 0.02)
]

# top_p=0.9（保留累计概率90%的词）
保留：["好", "不错", "真好", "挺好"]  # 4个词
丢弃：["棒", "赞"]  # 概率太低
```

**与top_k的区别：**

```python
# 场景1：概率分布集中
概率 = [0.6, 0.2, 0.1, 0.05, 0.03, 0.02]

top_k=3:  保留前3个 [0.6, 0.2, 0.1]  # 固定3个
top_p=0.9: 保留到0.9 [0.6, 0.2, 0.1] # 也是3个

# 场景2：概率分布平坦
概率 = [0.15, 0.14, 0.13, 0.12, 0.11, 0.10, 0.09, 0.08, 0.08]

top_k=3:  保留前3个 [0.15, 0.14, 0.13]  # 总概率42%，损失多样性
top_p=0.9: 保留前8个 [..., 0.08]        # 更灵活 ✅
```

**推荐值：**

| 值 | 效果 | 适用场景 |
|---|------|---------|
| 0.5-0.7 | 保守 | 事实性内容 |
| **0.85-0.95** | **平衡** | **日常对话** ⭐ |
| 0.95-1.0 | 多样 | 创意写作 |

### 3.4 top_k（Top-K采样） ⭐⭐⭐

**作用：** 只从概率最高的K个词中采样

```python
top_k=50  # 只考虑概率最高的50个候选词

# 示例
所有词汇：30000个
top_k=50: 只从前50个中选择
```

**参数范围：**

| 值 | 效果 | 特点 |
|---|------|------|
| 1 | 贪心 | 总是选最高概率（等同do_sample=False） |
| 10-20 | 保守 | 选择范围小，输出保守 |
| **40-60** | **平衡** | **推荐** ⭐ |
| 100+ | 多样 | 选择范围大 |
| 0/None | 无限制 | 从所有词中选择 |

**top_k vs top_p：**

```python
# 推荐组合
top_k=50      # 先限制候选词数量
top_p=0.9     # 再根据概率动态筛选
# 两者结合，效果更好 ✅

# 单独使用
top_k=50, top_p=1.0      # 只用top_k
top_k=0,  top_p=0.9      # 只用top_p
```

### 3.5 采样策略对比

```python
# 策略1：贪心解码（最确定）
do_sample=False
# 每次选概率最高的词，输出固定

# 策略2：温度采样
do_sample=True
temperature=0.7
# 根据温度调整后的概率分布采样

# 策略3：Top-K采样
do_sample=True
top_k=50
# 只从前50个候选词中采样

# 策略4：核采样（Top-P）
do_sample=True
top_p=0.9
# 动态选择累计概率达到90%的词

# ⭐ 策略5：组合策略（推荐）
do_sample=True
temperature=0.7      # 调整随机性
top_p=0.9           # 动态选择候选集
top_k=50            # 限制最大候选数
# 三者结合，效果最佳 ✅
```

---

## 4. 质量控制参数

### 4.1 repetition_penalty（重复惩罚） ⭐⭐⭐⭐

**作用：** 惩罚已经出现过的token，避免重复

**原理：** 降低已生成token的概率

```python
repetition_penalty=1.1  # 重复词概率 ÷ 1.1

# 示例
已生成："今天天气很好，天气"
下一个词概率：
  原始：{"很": 0.3, "天气": 0.5, "真": 0.2}  # "天气"已出现2次
  
  应用惩罚后（1.1）：
  {"很": 0.3, "天气": 0.45, "真": 0.2}  # "天气"概率降低
```

**参数范围：**

| 值 | 效果 | 适用场景 |
|---|------|---------|
| 1.0 | 无惩罚 | 允许重复 |
| **1.05-1.15** | **轻度惩罚** | **日常对话** ⭐ |
| 1.2-1.3 | 中度惩罚 | 避免明显重复 |
| 1.5+ | 强惩罚 | 诗歌、创意写作 |
| 2.0+ | 过度惩罚 | ❌ 可能导致不自然 |

**实际效果：**

```python
输入："介绍一下Python"

repetition_penalty=1.0:
"Python是一种编程语言，Python非常流行，Python易于学习..."
# ❌ 重复"Python"太多

repetition_penalty=1.1:
"Python是一种编程语言，它非常流行，并且易于学习..."
# ✅ 使用"它"代替重复的"Python"

repetition_penalty=2.0:
"Python乃编程之语，其备受青睐，学习门槛甚低..."
# ❌ 过度避免重复，表达不自然
```

### 4.2 no_repeat_ngram_size

**作用：** 禁止重复的n-gram（n个连续词）

```python
no_repeat_ngram_size=3  # 禁止3个连续词重复

# 示例
已生成："今天天气很好"
禁止再出现："今天天气很好"（3-gram重复）

已生成："我喜欢编程，我喜欢"
禁止下一个词："编程"（因为"我喜欢编程"会重复）
```

**推荐值：**
- 0：不启用
- 2-3：防止短语重复 ⭐
- 4+：过于严格

---

## 5. 束搜索参数

### 5.1 num_beams（束搜索宽度）

**作用：** 同时保留多个候选序列，选择总概率最高的

```python
# num_beams=1（贪心/采样）
每步只保留1个最佳候选

# num_beams=5（束搜索）
每步保留5个候选序列，最终选最优的
```

**原理：**

```python
输入："今天"

步骤1：
  候选1："今天 天气" (概率0.3)
  候选2："今天 很" (概率0.25)
  候选3："今天 是" (概率0.2)
  候选4："今天 我" (概率0.15)
  候选5："今天 你" (概率0.1)
  保留这5个

步骤2：对每个候选继续扩展
  候选1："今天 天气 很好" (总概率0.3×0.7=0.21)
  候选2："今天 很 高兴" (总概率0.25×0.6=0.15)
  ...
  再次选出前5个

最终：选择总概率最高的完整序列
```

**特点：**

| num_beams | 效果 | 质量 | 速度 | 多样性 |
|-----------|------|------|------|-------|
| 1 | 贪心/采样 | 一般 | 快 | 高（采样时） |
| 3-5 | 束搜索 | 更高 | 慢3-5倍 | 低 |
| 10+ | 宽束搜索 | 最高 | 很慢 | 很低 |

**注意：**
```python
# ⚠️ num_beams > 1 时，do_sample会被忽略
num_beams=5
do_sample=True  # 无效，束搜索不使用采样

# 推荐
# 对话应用：num_beams=1, do_sample=True  ✅ 多样性
# 翻译任务：num_beams=5, do_sample=False ✅ 准确性
```

---

## 6. 特殊Token参数

### 6.1 eos_token_id（结束标记）

**作用：** 生成到此token时停止

```python
eos_token_id=tokenizer.eos_token_id  # 通常是 </s> 或 <|endoftext|>

# 示例
生成："今天天气很好。</s>"
         └── 遇到eos_token_id，停止生成
```

### 6.2 pad_token_id（填充标记）

**作用：** 批量处理时用于对齐长度

```python
pad_token_id=tokenizer.pad_token_id

# 批量生成
输入1："你好" → [101, 102, 0, 0]  # 用pad_token(0)填充
输入2："你好吗" → [101, 102, 103, 0]
```

---

## 7. 参数组合策略

### 7.1 策略1：对话应用（平衡） ⭐

```python
# 适用：日常对话、聊天机器人
generation_kwargs = dict(
    max_new_tokens=2048,
    do_sample=True,
    temperature=0.7,          # 适度随机
    top_p=0.9,                # 动态选择
    top_k=50,                 # 限制范围
    repetition_penalty=1.1,   # 轻度防重复
    num_beams=1,              # 不用束搜索
)
# 特点：自然、多样、流畅
```

### 7.2 策略2：事实问答（准确）

```python
# 适用：知识问答、信息检索
generation_kwargs = dict(
    max_new_tokens=512,
    do_sample=True,
    temperature=0.3,          # 更确定 ✅
    top_p=0.85,               # 保守选择
    top_k=30,                 # 更少候选
    repetition_penalty=1.05,
    num_beams=1,
)
# 特点：准确、可靠、保守
```

### 7.3 策略3：创意写作（多样）

```python
# 适用：故事创作、诗歌生成
generation_kwargs = dict(
    max_new_tokens=4096,
    do_sample=True,
    temperature=0.9,          # 更随机 ✅
    top_p=0.95,               # 更多可能
    top_k=80,                 # 更大范围
    repetition_penalty=1.2,   # 强制多样性
    num_beams=1,
)
# 特点：创意、新颖、多变
```

### 7.4 策略4：代码生成（精确）

```python
# 适用：代码补全、代码生成
generation_kwargs = dict(
    max_new_tokens=1024,
    do_sample=False,          # 不采样 ✅
    temperature=0.2,          # 非常确定
    num_beams=1,
    repetition_penalty=1.0,   # 代码允许重复
)
# 特点：确定、精确、可执行
```

### 7.5 策略5：翻译任务（最优）

```python
# 适用：机器翻译
generation_kwargs = dict(
    max_new_tokens=512,
    do_sample=False,
    num_beams=5,              # 使用束搜索 ✅
    early_stopping=True,
    repetition_penalty=1.0,
)
# 特点：准确、流畅、最优解
```

---

## 8. 参数调优实战技巧

### 8.1 问题1：输出太保守、千篇一律

```python
# 症状
每次回答都很相似，缺乏变化

# 解决方案
temperature=0.7 → 0.9      # 增加随机性 ✅
top_p=0.9 → 0.95           # 扩大候选范围
top_k=50 → 80              # 增加选择
```

### 8.2 问题2：输出胡言乱语、不连贯

```python
# 症状
输出内容天马行空，逻辑混乱

# 解决方案
temperature=1.5 → 0.7      # 降低随机性 ✅
top_p=0.95 → 0.85          # 缩小候选范围
top_k=100 → 50             # 减少选择
```

### 8.3 问题3：大量重复内容

```python
# 症状
"Python是一种语言，Python很流行，Python易学，Python..."

# 解决方案
repetition_penalty=1.1 → 1.3        # 增加惩罚 ✅
no_repeat_ngram_size=0 → 3          # 禁止3-gram重复
temperature=0.5 → 0.8               # 增加多样性
```

### 8.4 问题4：回答太短或太长

```python
# 太短
max_new_tokens=256 → 1024           # 增加上限 ✅
min_length=0 → 100                  # 设置下限

# 太长
max_new_tokens=4096 → 1024          # 降低上限 ✅
# 或在prompt中明确要求："请用100字以内回答"
```

### 8.5 问题5：生成速度慢

```python
# 症状
生成很慢，用户等待时间长

# 解决方案
num_beams=5 → 1                     # 关闭束搜索 ✅
max_new_tokens=4096 → 2048          # 减少生成长度
# 或使用量化模型（Int4/Int8）
```

---

## 9. 参数快速参考

### 9.1 完整参数对照表

| 参数 | 类型 | 范围 | 默认值 | 优先级 | 说明 |
|------|------|------|--------|--------|------|
| **max_new_tokens** | 长度 | 1-8192 | 2048 | ⭐⭐⭐⭐⭐ | 最大生成长度 |
| **temperature** | 采样 | 0.1-2.0 | 0.7 | ⭐⭐⭐⭐⭐ | 随机性控制 |
| **top_p** | 采样 | 0.0-1.0 | 0.9 | ⭐⭐⭐⭐⭐ | 核采样阈值 |
| **top_k** | 采样 | 0-100 | 50 | ⭐⭐⭐⭐ | Top-K采样 |
| **repetition_penalty** | 质量 | 1.0-2.0 | 1.1 | ⭐⭐⭐⭐ | 重复惩罚 |
| **do_sample** | 采样 | True/False | True | ⭐⭐⭐⭐⭐ | 采样开关 |
| **num_beams** | 搜索 | 1-10 | 1 | ⭐⭐⭐ | 束搜索宽度 |
| no_repeat_ngram | 质量 | 0-5 | 0 | ⭐⭐⭐ | N-gram重复 |
| max_length | 长度 | 1-8192 | 4096 | ⭐⭐ | 总长度 |
| min_length | 长度 | 0-1000 | 0 | ⭐⭐ | 最小长度 |

### 9.2 常见场景速查

| 场景 | temp | top_p | top_k | rep_pen | beams | 特点 |
|------|------|-------|-------|---------|-------|------|
| 日常对话 | 0.7 | 0.9 | 50 | 1.1 | 1 | 自然流畅 ⭐ |
| 事实问答 | 0.3 | 0.85 | 30 | 1.05 | 1 | 准确可靠 |
| 创意写作 | 0.9 | 0.95 | 80 | 1.2 | 1 | 多样新颖 |
| 代码生成 | 0.2 | 0.9 | 50 | 1.0 | 1 | 精确可用 |
| 机器翻译 | - | - | - | 1.0 | 5 | 最优翻译 |
| 头脑风暴 | 1.2 | 0.98 | 100 | 1.3 | 1 | 发散思维 |

### 9.3 调优流程图

```
开始调优
    ↓
设置基准参数（温度0.7，top_p 0.9）
    ↓
生成测试
    ↓
┌─────────┬─────────┬─────────┐
↓         ↓         ↓         ↓
太保守？  太随机？  重复多？  速度慢？
↓         ↓         ↓         ↓
增加temp  降低temp  增加rep   减少beams
增加top_p 降低top_p 增加ngram 降低长度
    ↓         ↓         ↓         ↓
    └─────────┴─────────┴─────────┘
                ↓
            达到满意效果
                ↓
              保存配置
```

### 9.4 黄金组合推荐 ⭐

```python
# 对话应用最佳配置
generation_kwargs = dict(
    max_new_tokens=2048,
    do_sample=True,
    temperature=0.7,          # 黄金温度
    top_p=0.9,                # 黄金采样阈值
    top_k=50,                 # 黄金候选数
    repetition_penalty=1.1,   # 黄金惩罚值
    num_beams=1,
)
# 这是经过大量实践验证的最佳组合！✅
```

---

## 总结

### 关键要点

1. **超参数 ≠ 模型参数**：超参数由用户设置，控制生成行为
2. **设置时机**：每次调用generate()时传递，不是模型加载时 ⭐
3. **三大核心参数**：temperature（随机性）、top_p（候选集）、top_k（候选数）
4. **黄金组合**：temp=0.7, top_p=0.9, top_k=50, rep_pen=1.1
5. **采样 > 贪心**：对话应用推荐 do_sample=True
6. **束搜索 ≠ 采样**：num_beams > 1 时不能使用采样
7. **事实标准**：HuggingFace Transformers接口被广泛采用
8. **调优原则**：从基准开始，针对问题逐步调整

### 快速上手

```python
# 第一步：了解何时设置参数
model = AutoModelForCausalLM.from_pretrained(...)  # 加载模型（不设置超参数）
output = model.generate(input_ids, temperature=0.7, ...)  # 生成时传递（✅）

# 第二步：使用黄金组合
temperature=0.7
top_p=0.9
top_k=50
repetition_penalty=1.1

# 第三步：根据效果微调
太保守？ → 增加 temperature
太随机？ → 降低 temperature
重复多？ → 增加 repetition_penalty

# 第四步：保存你的最佳配置
```

---

*最后更新：2025年10月*
